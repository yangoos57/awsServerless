{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from etl import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keybert import KeyBERT\n",
    "import pymysql\n",
    "import ast\n",
    "import re\n",
    "from gensim.models import Word2Vec,KeyedVectors\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "keyBertModel = KeyBERT(\"paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "\n",
    "conn=pymysql.connect(host='localhost',port=int(3306),user='root',passwd='',db='dash_test')\n",
    "cursor = conn.cursor(pymysql.cursors.DictCursor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract, Transform, and Load "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111003 완료 | 428 개 추출\n",
      "111011 완료 | 599 개 추출\n",
      "111007 완료 | 1267 개 추출\n",
      "111022 완료 | 0 개 추출111008 완료 | 0 개 추출\n",
      "\n",
      "111004 완료 | 0 개 추출\n",
      "111009 완료 | 0 개 추출\n",
      "111006 완료 | 0 개 추출\n",
      "111010 완료 | 0 개 추출\n",
      "111005 완료 | 0 개 추출\n",
      "111012 완료 | 590 개 추출\n",
      "111021 완료 | 388 개 추출\n",
      "111013 완료 | 996 개 추출\n",
      "111018 완료 | 574 개 추출\n",
      "111030 완료 | 531 개 추출\n",
      "111014 완료 | 0 개 추출\n",
      "111016 완료 | 411 개 추출\n",
      "111019 완료 | 0 개 추출111015 완료 | 0 개 추출\n",
      "\n",
      "111020 완료 | 0 개 추출\n",
      "Skip : 9791164262090\n",
      "Skip : 9791164262106\n",
      "kyoboBooks 추출완료\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#-- extract updated books of each library -- #\n",
    "rawbookinfo = extractAllLibBooksMultiThread('2022-08-12')\n",
    "df = rawbookinfo.drop_duplicates(subset=\"ISBN\")\n",
    "\n",
    "\n",
    "#-- load books info in DB --#\n",
    "cursor.execute(' SELECT ISBN FROM backend_dodomoabookinfo')\n",
    "result = cursor.fetchall()\n",
    "libinfo = pd.DataFrame(result)\n",
    "\n",
    "#-- compare extracted book info with previous book info and get new books   --#\n",
    "# extract ISBNs of new books and compare them with previous books\n",
    "ISBNs = df['ISBN'].tolist()\n",
    "BM = np.in1d(ISBNs,libinfo['ISBN'])\n",
    "\n",
    "\n",
    "# extract new book's info by crowaling kyobobooks site\n",
    "ISBNs = df[~BM]['ISBN']\n",
    "docs = kyoboSaveMultiThread(ISBNs, thread_num = 10)\n",
    "\n",
    "# merge the extracted texts with book info\n",
    "orderlist = [i[1] for i in docs]\n",
    "k = pd.DataFrame([orderlist,docs]).T\n",
    "k.columns = 'ISBN','keywords'\n",
    "\n",
    "# make them as a dataframe\n",
    "newdf = df[~BM]\n",
    "newdf = newdf.merge(k,left_on='ISBN',right_on='ISBN')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### extract book's keywords using keyBert "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate keywords and info to extract keywords\n",
    "df = newdf.drop(columns=['keywords'])\n",
    "\n",
    "# before extracting keywords\n",
    "docs = newdf['keywords']\n",
    "\n",
    "# dataframe with extracted keywords\n",
    "dfWithKeywords = transform((df,docs),keyBertModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### update BookInfo For training Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5281"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# back stringified list to list\n",
    "def changeStringToList(strList) :\n",
    "    return ast.literal_eval(strList)\n",
    "\n",
    "\n",
    "#-- transform and change column name --#\n",
    "a = list(map(changeStringToList, newdf['keywords']))\n",
    "newBookInfo = pd.DataFrame(a)\n",
    "newBookInfo.columns = [f'col{i}' for i in range(len(newBookInfo.columns))]\n",
    "\n",
    "\n",
    "#-- concat with old one and save --#\n",
    "# load a previous file (혹시 파일 잘못된 경우 bookinfo12.csv로 백업 해놨음)\n",
    "bookInfo = pd.read_csv('./data/newbookinfo12.csv',index_col=0)\n",
    "\n",
    "# concat new items with previous items\n",
    "concatNewBookInfo = pd.concat([bookInfo,newBookInfo])\n",
    "\n",
    "\n",
    "#-- drop duplicates by ISBN --#\n",
    "concatNewBookInfo['col1'] = concatNewBookInfo['col1'].astype(int)\n",
    "concatNewBookInfo = concatNewBookInfo.drop_duplicates(subset='col1')\n",
    "\n",
    "\n",
    "#-- save --#\n",
    "concatNewBookInfo.to_csv('./data/newbookinfo12.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load items to make same condtions \n",
    "bookinfo = pd.read_csv('./data/newbookinfo12.csv',index_col=0)\n",
    "\n",
    "# load corpus analyzer\n",
    "okt = Okt()\n",
    "\n",
    "# change rows to lists and join them in a string \n",
    "def mergeListToString(item:pd.Series) :\n",
    "    wordList = item.astype(str).tolist()\n",
    "    str_list = [re.sub('\\d','',str(a)) for a in wordList]\n",
    "    str_list = list(filter(None, str_list))\n",
    "    result = ' '.join(str_list)\n",
    "    return result\n",
    "\n",
    "# iterate all rows of bookinfo\n",
    "wordsList=[mergeListToString(i[1]) for i in bookinfo.iterrows()]\n",
    "print('Complete wordsList Load!!')\n",
    "\n",
    "# analyze corpus\n",
    "print('konelpy 실행 중... 평균 9분 소요')\n",
    "konlpyWords = list(map(lambda x : okt.nouns(x),wordsList ))\n",
    "print('Complete konlpyWords')\n",
    "\n",
    "# train Word2vec\n",
    "embedding_model = Word2Vec(sentences=konlpyWords, window = 2, min_count=50, workers=7, sg=1)\n",
    "\n",
    "# save\n",
    "embedding_model.wv.save_word2vec_format('w2v') \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('컴포즈', 0.7917843461036682),\n",
       " ('컨테이너', 0.7492018342018127),\n",
       " ('컨테이너화', 0.7179580926895142),\n",
       " ('쿠버네티스', 0.7150908708572388),\n",
       " ('스웜', 0.7100293636322021),\n",
       " ('오케스트레이션', 0.6814238429069519),\n",
       " ('메소스', 0.6800644993782043),\n",
       " ('앤서블', 0.6697981357574463),\n",
       " ('레지스트리', 0.6672812700271606),\n",
       " ('젠킨스', 0.6596615314483643)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract 20 keywords related to a word.\n",
    "loaded_model = KeyedVectors.load_word2vec_format(\"w2v\") \n",
    "keywordsWord2Vec = loaded_model.most_similar(positive=['도커'],topn=10)\n",
    "keywordsWord2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymysql\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "#sqlalchemy 연결 \n",
    "db_connection_str = 'mysql+pymysql://root@localhost:3306/dash_test'\n",
    "db_connection = create_engine(db_connection_str)\n",
    "conn = db_connection.connect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### backend_dodomoalibinfo\n",
    "\n",
    "* Libinfo has `ISBN` and `지역` column.\n",
    "\n",
    "* This table is used for searching books of libraries that user selects .\n",
    "* ISBN is id of books, so it is easy to get info at other tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select ISBN and 지역 column of rawbookinfo\n",
    "booklist = rawbookinfo[['ISBN','지역']]\n",
    "\n",
    "\n",
    "# save to backend_dodomoalibinfo table\n",
    "booklist.to_sql(name='backend_dodomoalibinfo', con=db_connection, if_exists='append',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### backend_dodomoabookinfo\n",
    "* Bookinfo has `도서명`,`저자`,`출판사`,`ISBN`,`주제분류번호`,`등록일자`,`이미지주소` column.\n",
    "\n",
    "* This has books of all libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# newdf = pd.read_csv('./data/tempdata.csv',index_col=0)\n",
    "backend_dodomoabookinfo = newdf[['도서명','저자','출판사','ISBN','주제분류번호','등록일자','이미지주소']]\n",
    "\n",
    "backend_dodomoabookinfo['출판사'] = backend_dodomoabookinfo['출판사'].fillna('-')\n",
    "backend_dodomoabookinfo['주제분류번호'] = '00'+ backend_dodomoabookinfo['주제분류번호'].astype(str)\n",
    "\n",
    "backend_dodomoabookinfo.to_sql(name='backend_dodomoabookinfo', con=db_connection, if_exists='append',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### backend_dodomoakeyword2\n",
    "* Keyword2 has `ISBN` and `keywords` column.\n",
    "\n",
    "* all keywords are joined and saved as a string.\n",
    "* keywords column has fulltext index\n",
    "* This table returns ISBNs that match with user search keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backend_dodomoakeyword2 = dfWithKeywords[['ISBN','keywords']]\n",
    "backend_dodomoakeyword2['keywords'] = list(map(lambda x : ' '.join(x),backend_dodomoakeyword2['keywords']))\n",
    "\n",
    "backend_dodomoakeyword2.columns = ['ISBN','keyword']\n",
    "\n",
    "backend_dodomoakeyword2.to_sql(name='backend_dodomoakeyword2', con=db_connection, if_exists='append',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b2097164ba635ebffc0e3795dc845ae25b57eedf0c1eb5773ded6aee9fc1b279"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
