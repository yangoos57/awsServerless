{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentencebert를 활용해 문서 키워드 추출하기\n",
    "\n",
    "* Sentence Bert를 활용해 도서 목차, 도서 소개, 추천사 등의 도서 정보를 하나의 sentnece embedding으로 표현한 뒤\n",
    "\n",
    "* 도서 정보에서 빈번하게 나오는 단어를 순서대로 추려 word embedding으로 표현하고\n",
    "\n",
    "* 최종적으로 sentence embedding과 word embedding 간 Cosine Similarity를 계산하여 \n",
    "\n",
    "* 문서 키워드를 추출함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at monologg/koelectra-base-v3-discriminator were not used when initializing ElectraModel: ['discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight']\n",
      "- This IS expected if you are initializing ElectraModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import ElectraModel, ElectraTokenizerFast\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from model import SentenceBert\n",
    "from konlpy.tag import Hannanum\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "model = ElectraModel.from_pretrained(\"monologg/koelectra-base-v3-discriminator\")\n",
    "tokenizer = ElectraTokenizerFast.from_pretrained(\"monologg/koelectra-base-v3-discriminator\")\n",
    "\n",
    "Sbert = SentenceBert(model, tokenizer)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data 불러오기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>isbn13</th>\n",
       "      <th>title</th>\n",
       "      <th>toc</th>\n",
       "      <th>intro</th>\n",
       "      <th>publisher</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9791163034254</td>\n",
       "      <td>깡샘의 안드로이드 앱 프로그래밍 with 코틀린</td>\n",
       "      <td>[첫째마당 안드로이드 앱 개발 준비하기, 개발 환경 준비하기, 안드로이드 스튜디오 ...</td>\n",
       "      <td>[안드로이드 코틀린 분야 위 도서였던 개정판에 이어 개정 판이 출간되었다, 이번 판...</td>\n",
       "      <td>[이 책의 특징 안드로이드 티라미수을 기준으로 내용 및 소스를 업데이트했습니다, 전...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9788966263677</td>\n",
       "      <td>CPython 파헤치기</td>\n",
       "      <td>[소스 코드에 포함된 것들, 장 개발 환경 구성하기, 편집기와 통합 개발 환경, 비...</td>\n",
       "      <td>[파이썬이 인터프리터 레벨에서 작동하는 방식을 이해하면 파이썬의 기능을 최대한 활용...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9791140702473</td>\n",
       "      <td>부모님을 위한 컴퓨터 무작정 따라하기</td>\n",
       "      <td>[첫째 마당 이것만은 꼭 컴퓨터 기초 지식 다지기, 컴퓨터 자기소개, 컴퓨터 기본 ...</td>\n",
       "      <td>[컴퓨터 앞에 서면 막막해진다는 부모님을 위해 시작한 욜디 의 컴퓨터 기초 강의 이...</td>\n",
       "      <td>[세상 쉬운부모님을 위한 컴퓨터 무작정 따라하기더이상 물어볼 필요 없어요 일상에 힘...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9791192932057</td>\n",
       "      <td>챗GPT</td>\n",
       "      <td>[AI는 이미 당신보다 똑똑하다, 너무 똑똑한 AI의 출현 위기인가 기회인가, 고도...</td>\n",
       "      <td>[출시된 지 얼마 되지도 않아 세상을 뒤흔든 챗GPT는 지금까지 나온 모든 인공지능...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9791140702787</td>\n",
       "      <td>쉽게 시작하는 쿠버네티스</td>\n",
       "      <td>[장 쿠버네티스의 등장, 컨테이너 환경으로의 진화, 쿠버네티스를 학습하기 전에 알아...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[모든 것은 기본에서 시작한다 가볍지만 알차게 배우는 쿠버네티스 쿠버네티스는 컨테이...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          isbn13                       title  \\\n",
       "0  9791163034254  깡샘의 안드로이드 앱 프로그래밍 with 코틀린   \n",
       "1  9788966263677                CPython 파헤치기   \n",
       "2  9791140702473        부모님을 위한 컴퓨터 무작정 따라하기   \n",
       "3  9791192932057                        챗GPT   \n",
       "4  9791140702787               쉽게 시작하는 쿠버네티스   \n",
       "\n",
       "                                                 toc  \\\n",
       "0  [첫째마당 안드로이드 앱 개발 준비하기, 개발 환경 준비하기, 안드로이드 스튜디오 ...   \n",
       "1  [소스 코드에 포함된 것들, 장 개발 환경 구성하기, 편집기와 통합 개발 환경, 비...   \n",
       "2  [첫째 마당 이것만은 꼭 컴퓨터 기초 지식 다지기, 컴퓨터 자기소개, 컴퓨터 기본 ...   \n",
       "3  [AI는 이미 당신보다 똑똑하다, 너무 똑똑한 AI의 출현 위기인가 기회인가, 고도...   \n",
       "4  [장 쿠버네티스의 등장, 컨테이너 환경으로의 진화, 쿠버네티스를 학습하기 전에 알아...   \n",
       "\n",
       "                                               intro  \\\n",
       "0  [안드로이드 코틀린 분야 위 도서였던 개정판에 이어 개정 판이 출간되었다, 이번 판...   \n",
       "1  [파이썬이 인터프리터 레벨에서 작동하는 방식을 이해하면 파이썬의 기능을 최대한 활용...   \n",
       "2  [컴퓨터 앞에 서면 막막해진다는 부모님을 위해 시작한 욜디 의 컴퓨터 기초 강의 이...   \n",
       "3  [출시된 지 얼마 되지도 않아 세상을 뒤흔든 챗GPT는 지금까지 나온 모든 인공지능...   \n",
       "4                                                 []   \n",
       "\n",
       "                                           publisher  \n",
       "0  [이 책의 특징 안드로이드 티라미수을 기준으로 내용 및 소스를 업데이트했습니다, 전...  \n",
       "1                                                 []  \n",
       "2  [세상 쉬운부모님을 위한 컴퓨터 무작정 따라하기더이상 물어볼 필요 없어요 일상에 힘...  \n",
       "3                                                 []  \n",
       "4  [모든 것은 기본에서 시작한다 가볍지만 알차게 배우는 쿠버네티스 쿠버네티스는 컨테이...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "raw_data = pd.read_parquet(\"../../App/db/data/book_scraping.parquet\")\n",
    "\n",
    "raw_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text, max_length=128):\n",
    "    token = tokenizer(\n",
    "        text,\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=max_length,\n",
    "        stride=20,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    token.pop(\"overflow_to_sample_mapping\")\n",
    "    return token\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 문서 키워드 추출\n",
    "\n",
    "* 1단계. max_length개 토큰 이상으로 구성된 문서인 경우 여러 문단으로 나눔.\n",
    "\n",
    "* 2단계. 문서 내 한글 및 영문 키워드 추출\n",
    "* 3단계. Sbert를 활용해 문서 및 키워드 embedding\n",
    "* 4단계. 문서와 키워드 간 cosine similarity 측정\n",
    "* 5단계. 유사도 높은 단어 순으로 키워드 제공"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "변환한 도서정보 :  그림과 실습으로 배우는 도커 & 쿠버네티스\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>유사도</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>메타데이터</th>\n",
       "      <td>0.942622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>virtualbox</th>\n",
       "      <td>0.922685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>쿠버네티스</th>\n",
       "      <td>0.919135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>리눅스</th>\n",
       "      <td>0.901917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mysql</th>\n",
       "      <td>0.900849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>프롬프트</th>\n",
       "      <td>0.898217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>레드마</th>\n",
       "      <td>0.895093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>리눅스용</th>\n",
       "      <td>0.894459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>디플로이먼트</th>\n",
       "      <td>0.893091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macos</th>\n",
       "      <td>0.890312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>도커</th>\n",
       "      <td>0.889823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>컴포즈</th>\n",
       "      <td>0.888451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>준비</th>\n",
       "      <td>0.882835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>실습</th>\n",
       "      <td>0.881106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>삭제</th>\n",
       "      <td>0.879038</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 유사도\n",
       "메타데이터       0.942622\n",
       "virtualbox  0.922685\n",
       "쿠버네티스       0.919135\n",
       "리눅스         0.901917\n",
       "mysql       0.900849\n",
       "프롬프트        0.898217\n",
       "레드마         0.895093\n",
       "리눅스용        0.894459\n",
       "디플로이먼트      0.893091\n",
       "macos       0.890312\n",
       "도커          0.889823\n",
       "컴포즈         0.888451\n",
       "준비          0.882835\n",
       "실습          0.881106\n",
       "삭제          0.879038"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import utils \n",
    "\n",
    "def keyword_extraction_from_doc(\n",
    "    doc: str, sbert, model,min_num=3, num_rank=20, max_length=128, noun_extractor=Hannanum()\n",
    "):\n",
    "    \"\"\"\n",
    "    *-- arguments --*\n",
    "\n",
    "    doc : 문서 정보\n",
    "    sbert : sbert 모델\n",
    "    min_num : 키워드 최소 출현 횟수 ex) 문서 내 3회 이상 사용된 경우 추출\n",
    "    min_rank : 키워드 순위 설정\n",
    "    max_length : 문장 Tokenizing 범위\n",
    "    noun_extractor : 한글 명사 추출을 위해 Konlpy 활용\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Doc embedding\n",
    "    # Doc Token이 128개 이상인 경우 여러 문장으로 구분\n",
    "    # ex) 1280개 토큰이 있는 Doc인 경우 128개 토큰이 있는 문장 10개로 구분\n",
    "    token = tokenize_text(doc, max_length=max_length)\n",
    "\n",
    "    ### 도서 정보를 나타내는 Sentence Embedding 생성(10개라면 10개 생성)\n",
    "    logits = sbert(**token)[\"sentence_embedding\"]\n",
    "\n",
    "    ### Keyword 후보 추출 추출\n",
    "\n",
    "    # 한글 키워드 추출\n",
    "    if type(doc) == str:\n",
    "        han_nouns = noun_extractor.nouns(doc)\n",
    "    else:\n",
    "        raise TypeError(\"doc must be str type.\")\n",
    "\n",
    "    candidates_kor = pd.DataFrame(han_nouns)[0].value_counts()\n",
    "    candidate_kor_words = candidates_kor[candidates_kor >= min_num].index.values.tolist()\n",
    "    candidate_kor_words = [i for i in candidate_kor_words if len(i) > 1]\n",
    "\n",
    "    # 영문 키워드 추출\n",
    "    book_info_eng = utils.find_eng(doc, min_num=0)\n",
    "\n",
    "    if book_info_eng:\n",
    "        candidates_eng = pd.DataFrame(book_info_eng)[0].value_counts()\n",
    "        candidates_eng_words = candidates_eng[candidates_eng >= min_num].index.values.tolist()\n",
    "    else:\n",
    "        candidates_eng_words = []\n",
    "\n",
    "    # 키워드 총합\n",
    "    candidate_words = candidate_kor_words + candidates_eng_words\n",
    "\n",
    "    if candidate_words == False:\n",
    "        return pd.DataFrame(columns=[\"유사도\"])\n",
    "\n",
    "    # 키워드에 대한 embedding 구하기\n",
    "    token_embedding = tokenizing_function(candidate_words)\n",
    "    last_hidden_state = model(**token_embedding)[\"last_hidden_state\"]\n",
    "\n",
    "    ### 문서 embedding\n",
    "    # [CLS], [SEP] 제거 ([CLS], [SEP]을 제거하면 정확도가 올라감.)\n",
    "    attention_mask = token_embedding[\"attention_mask\"]\n",
    "    for i in range(attention_mask.size(0)):\n",
    "        # x = attention mask 1에 포함 된 마지막 index\n",
    "        x = (attention_mask[i] == 1).nonzero(as_tuple=True)[0][-1]\n",
    "        attention_mask[i][0] = 0  # [CLS] = 0\n",
    "        attention_mask[i][x] = 0  # [SEP] = 0\n",
    "\n",
    "    # 토큰 내 padding 찾기 = [batch_size, src_token, embed_size]\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "\n",
    "    # padding인 경우 0 아닌 경우 1을 곱함 = [batch_size, embed_size]\n",
    "    sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n",
    "\n",
    "    # 평균을 위한 token 개수 확대\n",
    "    sum_mask = input_mask_expanded.sum(1)\n",
    "    sum_mask = torch.clamp(sum_mask, min=1e-9)\n",
    "\n",
    "    # Mean Pooling\n",
    "    result = sum_embeddings / sum_mask\n",
    "\n",
    "    ### Keyword와 Doc 문장 비교\n",
    "    sentence_comparsion = []\n",
    "    len_sentences = logits.size(0)\n",
    "\n",
    "    # sentence embedding별 키워드 비교\n",
    "    for i in range(len_sentences):\n",
    "        sentence_comparsion.append(\n",
    "            cosine_similarity(logits[i].unsqueeze(0).detach(), result.detach())\n",
    "        )\n",
    "\n",
    "    # Max Pooling\n",
    "    # 하나의 문단이 여러개로 나눠지게 되면 나눠진 개수 만큼 문장과 단어 간 cosine_similiarty를 계산함.\n",
    "    # 만약 1개의 문단이 10개의 문단으로 나뉘면 단어 하나 당 10개의 cosine_smiliarity가 존재하는 것임. \n",
    "    # 10개의 cosine_similarity 중 max값을 채택하겠다는 의미임.\n",
    "    result = np.max(sentence_comparsion, axis=0)  # Max\n",
    "\n",
    "    # Ranking 정리\n",
    "    result = pd.DataFrame(result.T, index=candidate_words, columns=[\"유사도\"]).sort_values(\n",
    "        by=\"유사도\", ascending=False\n",
    "    )[:num_rank]\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "from random import randrange\n",
    "\n",
    "# 도서 Random Sampling\n",
    "num = randrange(len(raw_data))\n",
    "num = 3 \n",
    "book_info: str = utils.merge_series_to_str(raw_data.iloc[num])\n",
    "\n",
    "# 영단어를 한글로 변환 ex) Python => 파이썬\n",
    "englist = pd.read_csv(\"../data/preprocess/englist.csv\")\n",
    "book_info_trans = utils.trans_eng_to_han(book_info, englist=englist)\n",
    "\n",
    "\n",
    "keyword_extraction_from_doc(\" \".join(book_info_trans), Sbert, model,num_rank=15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b2097164ba635ebffc0e3795dc845ae25b57eedf0c1eb5773ded6aee9fc1b279"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
